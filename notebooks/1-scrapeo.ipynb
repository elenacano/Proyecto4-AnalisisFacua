{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping y recolección de datos de la web de Facua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tratamiento de datos\n",
    "# -----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Importar librerías para automatización de navegadores web con Selenium\n",
    "# -----------------------------------------------------------------------\n",
    "from selenium import webdriver  # Selenium es una herramienta para automatizar la interacción con navegadores web.\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # ChromeDriverManager gestiona la instalación del controlador de Chrome.\n",
    "from selenium.webdriver.common.keys import Keys  # Keys es útil para simular eventos de teclado en Selenium.\n",
    "from selenium.webdriver.support.ui import Select  # Select se utiliza para interactuar con elementos <select> en páginas web.\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException # Excepciones comunes de selenium que nos podemos encontrar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso de este proyecto es obtener para cada uno de los supermercados de la web de facua el precio a lo largo del tiempo de tres tipo de productos: aceite de giralos, aceite de oliva y leche. Para ello usaremos Beautiful soup y selenium: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "url=\"https://super.facua.org/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "sleep(1)\n",
    "\n",
    "# Diccionario donde por super y categoría almacenaremos los links de las páginas de los productos\n",
    "diccionario_url = {\n",
    "    \"mercadona\":{},\n",
    "    \"carrefour\":{},\n",
    "    \"eroski\":{},\n",
    "    \"dia\":{},\n",
    "    \"hipercor\":{},\n",
    "    \"alcampo\":{},\n",
    "}\n",
    "\n",
    "# Itero por supermercado\n",
    "for i in range(1,7):\n",
    "    sleep(1)\n",
    "    driver.find_element(\"css selector\", f\"body > section:nth-child(4) > div > div.row.gx-4.gx-lg-6.row-cols-2.row-cols-md-2.row-cols-xl-6.justify-content-center > div:nth-child({i}) > div > div.card-footer.p-4.pt-0.border-top-0.bg-transparent > div > a\").click()\n",
    "    sleep(2)\n",
    "\n",
    "    aux_elements=driver.find_elements(\"css selector\", \"body > section:nth-child(4) > div > div.row.gx-4.gx-lg-5.row-cols-2.row-cols-md-3.row-cols-xl-4.justify-content-center\")\n",
    "    aux = [element.text for element in aux_elements]\n",
    "    lista_productos = aux[0].split(\"\\n\")[::2]\n",
    "    \n",
    "    j=1\n",
    "    for producto in lista_productos:\n",
    "        \n",
    "        # -------------Aceite de girasol, aceite de oliva, leche------------------\n",
    "        driver.find_element(\"css selector\", f\"body > section:nth-child(4) > div > div.row.gx-4.gx-lg-5.row-cols-2.row-cols-md-3.row-cols-xl-4.justify-content-center > div:nth-child({j}) > div > div.card-footer.p-4.pt-0.border-top-0.bg-transparent > div > a\").click()\n",
    "        sleep(2)\n",
    "\n",
    "        #Saco los link de los productos de aceite de girasol\n",
    "        html_table_page = driver.page_source\n",
    "        sopa = BeautifulSoup(html_table_page, \"html.parser\")\n",
    "        productos = sopa.findAll(\"div\", {\"class\":\"card-footer p-4 pt-0 border-top-0 bg-transparent\"})\n",
    "        lista_links = []\n",
    "        for prod in productos:\n",
    "            link = prod.find(\"a\", {\"class\":\"btn-unirme btn-verde inline-block inline-block bg-primary border-primary font-semibold rounded-full\"}).get(\"href\")\n",
    "            lista_links.append(link)\n",
    "        \n",
    "        #Sacamos la clave correspondiente para el diccionario\n",
    "        clave = list(diccionario_url.keys())[i-1]\n",
    "        diccionario_url[clave][producto]=lista_links\n",
    "\n",
    "        driver.back()\n",
    "        sleep(1)\n",
    "        j+=1\n",
    "\n",
    "    driver.back()\n",
    "    sleep(2)\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datos/diccionario_links.json\", \"w\") as archivo:\n",
    "    json.dump(diccionario_url, archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datos/diccionario_links.json\", \"r\") as archivo:\n",
    "    diccionario_url = json.load(archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un dataframe para cada producto y supermercado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [04:30<00:00, 45.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# iteramos para cada super\n",
    "for clave, valor in tqdm(diccionario_url.items()):\n",
    "    #Iteramos para cada producto\n",
    "    for producto in lista_productos:\n",
    "        links = diccionario_url[clave][producto]\n",
    "        df_producto = pd.DataFrame()\n",
    "        for link in links:\n",
    "            # Meto un try excepy porque hay algunos links que no son de productos sino de una categoría\n",
    "            try:\n",
    "                url = link\n",
    "                contenido = requests.get(url)\n",
    "                contenido.status_code\n",
    "                sopa = BeautifulSoup(contenido.content, \"html.parser\")\n",
    "\n",
    "                tabla = sopa.find(\"table\", {\"class\":\"table table-striped table-responsive text-center\"})\n",
    "\n",
    "                enunciados = [enunciado.getText() for enunciado in tabla.findAll(\"th\")]\n",
    "\n",
    "                filas_tabla=[]\n",
    "                for fila in tabla.findAll(\"tr\"):\n",
    "                    fila_sucia= fila.findAll(\"td\")\n",
    "                    aux=[elem.text for elem in fila_sucia]\n",
    "                    filas_tabla.append(aux)\n",
    "                \n",
    "                df= pd.DataFrame(filas_tabla[1:], columns=enunciados)\n",
    "                df[\"supermercado\"] = clave\n",
    "                df[\"producto\"] = producto\n",
    "                df[\"nombre_producto\"] = url.split(\"/\")[-2].replace(\"-\",\" \")\n",
    "                df_producto = pd.concat([df_producto, df])\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        df_producto.to_csv(f\"../datos/df_{clave}_{producto}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
